{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1gzrHUm1xxg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import drive\n",
        "import heapq\n",
        "import json\n",
        "import gc\n",
        "import csv\n",
        "import sys\n",
        "porter = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will connect our notebook to the google drive and we can easily fetch data from there\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFLHSqSG9rMg",
        "outputId": "080df2fb-c5c6-4212-8c8b-4da7480df825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From here we are basically retrieving the stop words from the nltk library which are printed here\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8BroFSP96ZR",
        "outputId": "9af46b07-fb6a-4f75-c909-119f0761f38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will help us to perform regular expression operation and filter more data.\n",
        "def remove_symbols(line):\n",
        "    return re.sub('[^A-Za-z0-9\\s]+', '', line).lower()\n"
      ],
      "metadata": {
        "id": "Nz3xwVMt_LCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now as we are going to read the CSV file so we will need higher field limit then the default so we can extend that using below code\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqYssGn5__Qg",
        "outputId": "ba8f335d-bf08-4663-dd9c-4f6b4133725c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Through below code we are reaching to the directory in which we have the gutenberd_data.csv. This is for my drive may vary for others\n",
        "%cd /content/drive/MyDrive/Information\\ Retrieval\\ Material\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaFPQ7anBN_B",
        "outputId": "53cb6b76-deb5-465a-d5ee-7b4d48afbf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Information Retrieval Material\n",
            "gutenberg_data.csv  \u001b[0m\u001b[01;34mOutputs\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 100000"
      ],
      "metadata": {
        "id": "X3x7Bxa7EFqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BSBI ROUTINE WHICH HELPS US MAKING EFFICIENT POSTING LISTS\n",
        "def bsbi():\n",
        "  freq_dict = defaultdict(set)\n",
        "  with open('gutenberg_data.csv') as f:\n",
        "    # Using the below line next(f) we are skipping the new column header\n",
        "    next(f)\n",
        "    csv_file = csv.reader(f)\n",
        "    total_files = 0\n",
        "    i = 0\n",
        "    current_block = 0\n",
        "    for line in csv_file:\n",
        "      # we are extracting title, author,link, id, bookshelf, text from a particular line or entry in the CSV\n",
        "      title, author, link, id, bookshelf, text = line\n",
        "      i += 1\n",
        "\n",
        "      # Here we will process the words that comes under the text\n",
        "      for word in text.split():\n",
        "        # here we are using the routine \"remove_symbols\" made above that will remove some symbols from the word.\n",
        "        word = remove_symbols(word)\n",
        "\n",
        "        # if the word exist and it is not in stop_words then we will stem the word\n",
        "        if word and word not in stop_words:\n",
        "          word = porter.stem(word)\n",
        "          if word not in freq_dict:\n",
        "            # if word is not added before, we will increase block size too\n",
        "            current_block += 1\n",
        "          \"\"\"\n",
        "          note: __contains__ will just check if doc is already there.\n",
        "          It's not exactly needed as we are using set, but we are checking it to manage block size\n",
        "          also searching in sets is faster as they are hashed (while lists are not hashed)\n",
        "          \"\"\"\n",
        "          if not freq_dict[word].__contains__(id):\n",
        "            freq_dict[word].add(id)\n",
        "            current_block += 1\n",
        "        if current_block >= BLOCK_SIZE:\n",
        "          # LETS DO THE WRITE OPERATION\n",
        "          # We will now sort the list before writing according to the word_id\n",
        "          sorted_list = sorted(freq_dict.items(), key=lambda _: _[0]) # sorting by word_id\n",
        "\n",
        "          with open(f'./Outputs/op{total_files}.txt', 'w') as  f:\n",
        "            # json.dump(freq_dict, f, cls=SetEncoder)\n",
        "            for word_id, docs in sorted_list:\n",
        "              f.write(word_id)\n",
        "              for doc_id in docs:\n",
        "                f.write(f' {doc_id}')\n",
        "              f.write('\\n')\n",
        "          current_block = 0\n",
        "          freq_dict.clear()\n",
        "          total_files += 1\n",
        "          print(i, ' rows done')\n",
        "          # if total_files == 2:\n",
        "          #   return\n",
        "\n",
        "    sorted_list = sorted(freq_dict.items(), key=lambda _: _[0]) # sorting by word_id\n",
        "    # this is for last values\n",
        "    # TODO:: DO IT BY FN SO NO REPEATATION OF CODE\n",
        "    if len(sorted_list) > 0:\n",
        "      with open(f'./Outputs/op{total_files}.txt', 'w') as  f:\n",
        "        # json.dump(freq_dict, f, cls=SetEncoder)\n",
        "        for word_id, docs in sorted_list:\n",
        "          f.write(word_id)\n",
        "          for doc_id in docs:\n",
        "            f.write(f' {doc_id}')\n",
        "          f.write('\\n')\n",
        "      current_block = 0\n",
        "      freq_dict.clear()\n",
        "      total_files += 1\n",
        "bsbi()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Bq_RbCyERIp",
        "outputId": "76f4c5a9-17e5-4f3f-cea2-cce87aae0300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15  rows done\n",
            "23  rows done\n",
            "41  rows done\n",
            "54  rows done\n",
            "67  rows done\n",
            "81  rows done\n",
            "96  rows done\n",
            "116  rows done\n",
            "130  rows done\n",
            "146  rows done\n",
            "157  rows done\n",
            "181  rows done\n",
            "207  rows done\n",
            "220  rows done\n",
            "254  rows done\n",
            "271  rows done\n",
            "280  rows done\n",
            "291  rows done\n",
            "302  rows done\n",
            "332  rows done\n",
            "353  rows done\n",
            "371  rows done\n",
            "387  rows done\n",
            "400  rows done\n",
            "444  rows done\n",
            "456  rows done\n",
            "468  rows done\n",
            "476  rows done\n",
            "488  rows done\n",
            "499  rows done\n",
            "506  rows done\n",
            "509  rows done\n",
            "518  rows done\n",
            "526  rows done\n",
            "533  rows done\n",
            "541  rows done\n",
            "548  rows done\n",
            "565  rows done\n",
            "590  rows done\n",
            "606  rows done\n",
            "616  rows done\n",
            "627  rows done\n",
            "634  rows done\n",
            "641  rows done\n",
            "649  rows done\n",
            "660  rows done\n",
            "678  rows done\n",
            "685  rows done\n",
            "693  rows done\n",
            "703  rows done\n",
            "718  rows done\n",
            "732  rows done\n",
            "740  rows done\n",
            "749  rows done\n",
            "757  rows done\n",
            "762  rows done\n",
            "772  rows done\n",
            "779  rows done\n",
            "788  rows done\n",
            "796  rows done\n",
            "804  rows done\n",
            "817  rows done\n",
            "828  rows done\n",
            "839  rows done\n",
            "848  rows done\n",
            "857  rows done\n",
            "866  rows done\n",
            "874  rows done\n",
            "885  rows done\n",
            "894  rows done\n",
            "905  rows done\n",
            "916  rows done\n",
            "924  rows done\n",
            "936  rows done\n",
            "954  rows done\n",
            "968  rows done\n",
            "980  rows done\n",
            "990  rows done\n",
            "1003  rows done\n",
            "1018  rows done\n",
            "1026  rows done\n",
            "1028  rows done\n",
            "1038  rows done\n",
            "1049  rows done\n",
            "1056  rows done\n",
            "1063  rows done\n",
            "1073  rows done\n",
            "1083  rows done\n",
            "1094  rows done\n",
            "1108  rows done\n",
            "1117  rows done\n",
            "1131  rows done\n",
            "1150  rows done\n",
            "1166  rows done\n",
            "1178  rows done\n",
            "1189  rows done\n",
            "1202  rows done\n",
            "1219  rows done\n",
            "1240  rows done\n",
            "1261  rows done\n",
            "1274  rows done\n",
            "1281  rows done\n",
            "1290  rows done\n",
            "1296  rows done\n",
            "1309  rows done\n",
            "1327  rows done\n",
            "1343  rows done\n",
            "1366  rows done\n",
            "1385  rows done\n",
            "1401  rows done\n",
            "1413  rows done\n",
            "1418  rows done\n",
            "1422  rows done\n",
            "1423  rows done\n",
            "1426  rows done\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9e22cc9d4d0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfreq_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mtotal_files\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mbsbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-9e22cc9d4d0c>\u001b[0m in \u001b[0;36mbsbi\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# here we are using the routine \"remove_symbols\" made above that will remove some symbols from the word.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# if the word exist and it is not in stop_words then we will stem the word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8497924b2f63>\u001b[0m in \u001b[0;36mremove_symbols\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This function will help us to perform regular expression operation and filter more data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^A-Za-z0-9\\s]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We had to do it on our local system so we ran this bsbi algo only upto 114 output files and we will be doing the rest on HADOOP\n",
        "file_names = [f'./Outputs/op{i}.txt' for i in range(114)]\n",
        "file_pointers = [open(i) for i in file_names]\n",
        "print(file_pointers)"
      ],
      "metadata": {
        "id": "UsL91yzDSiMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We will be merging the posting lists using the heap in python which is much more efficient. Link Below\n",
        "https://stackoverflow.com/questions/1001569/python-class-to-merge-sorted-files-how-can-this-be-improved\n",
        "\"\"\"\n",
        "\n",
        "# here we are using yeild so we are JUST READING ONE LINE at a time\n",
        "def decorated_file(f, key):\n",
        "  for line in f:\n",
        "    yield (key(line), line)\n",
        "\n",
        "files = map(open, file_names)\n",
        "outfile = open('./Output_2_Merged/merged.txt', 'w')\n",
        "\n",
        "def key_fn(line):\n",
        "    return line.split(' ', 2)[0] # returning word_id\n",
        "\n",
        "\"\"\"\n",
        "This is how we are merging the files using the heap in python\n",
        "\n",
        "Example:\n",
        "Assume Two files as shown below:\n",
        "\n",
        "File 1:\n",
        "\n",
        "Amaze 1 2 3\n",
        "Wow 3 4 5\n",
        "\n",
        "and\n",
        "\n",
        "File 2:\n",
        "\n",
        "Amaze 6 7 8\n",
        "Wow 8 9 10\n",
        "\n",
        "Output would be\n",
        "Amaze 1 2 3 6 7 8\n",
        "Wow 3 4 5 8 9 10\n",
        "\"\"\"\n",
        "\n",
        "prev = ''\n",
        "for line in heapq.merge(*[decorated_file(f, key_fn) for f in files]):\n",
        "  # to understand this, you can do this over small number of sorted files and try to print line\n",
        "  if prev != line[0]:\n",
        "    # if we have new word, make sure to add new line at first\n",
        "    outfile.write(f'\\n{line[1].strip()}')\n",
        "    prev = line[0]\n",
        "  # if we have same word yet, put a space and add other ids\n",
        "  else:\n",
        "    # line[1][len(line[0]):] => We are removing the word_id string and then writing the line\n",
        "    outfile.write(f' {line[1][len(line[0]):].strip()}')\n",
        "for i in file_pointers:\n",
        "  i.close()\n",
        "outfile.close()"
      ],
      "metadata": {
        "id": "2mZivIvJTOcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}